Below is a menu of lightweight, fast-running tests that catch the most common ROS 2 regressions before you ever power the robot. Pick the subset that matches each package’s role—­logic-heavy libraries get lots of pure unit tests, drivers get message/parameter tests, launch packages get launch-tests, etc.
Package	“Good” unit-test focus	Typical framework
omniseer_msgs (interfaces)	Compile & namespace tests
▪ Generated C++/Python types build & import
▪ rosidl_generate_interfaces finishes without warning	C++ GTest
Python import pytest
omniseer_hardware (C++)	Logic-only functions (CRC, kinematics) → GTest
Stub transport layer – inject fake CAN frames, assert outbound msg bytes
Parameter sanity – load YAML, ensure defaults parse	ament_add_gtest() + GoogleMock
omniseer_perception (Python)	Pre/post-processing helpers – NumPy in/out shapes
Model I/O – run one frame through PyTorch/TensorRT in CPU mode, assert tensor dims
Rate limiter – given timestamps, next publish time correct	pytest + pytest-benchmark
omniseer_control_cpp	PID maths – step-response settles < ε in N iterations
Safety limit – commanded velocity clamped to max even with overflow input	GTest with parameterised cases
omniseer_drl_policy (Python)	Action-space bounds – returned cmd_vel always within [-1,1]
Deterministic inference – fixed seed ⇒ identical action vector
Reward calc – synthetic state → expected scalar	pytest with fixtures, hypothesis property tests
omniseer_bringup (Python)	Launch tests – bring up graph, assert every node reaches active lifecycle state, topics exist, and parameter values propagated	launch_testing (ament)
1 Pure logic tests (no ROS)

Why: Fast (< 10 ms) and run on any CI runner.
C++

TEST(PID, settlesWithinTolerance) {
  PID pid(1.0, 0.1, 0.01);
  double out = 0;
  for (int i=0;i<100;i++)
    out = pid.update(1.0, 0.1);      // target, dt
  EXPECT_NEAR(out, 1.0, 0.02);
}

Add with:

ament_add_gtest(test_pid tests/test_pid.cpp)
ament_target_dependencies(test_pid pid_lib)

Python

def test_quaternion_norm():
    q = my_math.random_quaternion()
    assert math.isclose(np.linalg.norm(q), 1.0, rel_tol=1e-6)

2 Message & parameter round-trip tests

from sensor_msgs.msg import Imu
def test_imu_roundtrip():
    raw = Imu()
    raw.linear_acceleration.x = 1.2
    buf = raw.serialize()         # python type-support
    cooked = Imu()
    cooked.deserialize(buf)
    assert cooked.linear_acceleration.x == 1.2

Catch schema changes that would break bag replay or custom serialisers.
3 Node-in-isolation “spin” tests

Runs the node in its own rclcpp/rclpy executor but no DDS traffic.

TEST(LidarNode, publishesScanOnceTimerFires)
{
  rclcpp::init(0, nullptr);
  auto node = std::make_shared<LidarNode>();
  auto sub = node->create_subscription<LaserScan>(
     "scan", 10,
     [&](LaserScan::SharedPtr msg){ rclcpp::shutdown(); EXPECT_GT(msg->ranges.size(),0); });
  rclcpp::spin(node);
}

Execution time: ~20 ms. Good sanity check that timers, parameters, and publishers initialise.
4 Launch tests (graph-level)

import launch
import launch_ros.actions
import launch_testing
import pytest

@pytest.mark.launch_test
def generate_test_description():
    node = launch_ros.actions.Node(
        package='omniseer_hardware',
        executable='motor_driver_node',
        parameters=['test_params.yaml'])
    return [node], {
        'motor_node': node,
    }

def test_topics_ready(motor_node, proc_output):
    proc_output.assertWaitFor(
        'Motor driver node started', timeout=5)

    Use launch_testing_ament_cmake (C++) or launch_testing (Python).

    Assert nodes reach a lifecycle active state, topics exist (ros2 topic list), services advertised.

5 Hardware stub tests (driver packages)

    Replace the UART/CAN layer with a faux backend (GoogleMock, unittest.mock).

    Feed deterministic byte streams, assert published ROS messages equal ground truth.

    Ensures you never regress protocol framing or endianess.

6 Timing & throughput micro-benchmarks (optional)

    Use pytest-benchmark or custom LTTng trace cookies.

    Thresholds: “publish < 600 µs” or “< 40 % CPU @ 30 fps 1080p stream”.

    Fail the test if latency/CPU exceeds baseline by > N %.

7 Lint & style as blocking tests

<test_depend>ament_lint_auto</test_depend>
<test_depend>ament_lint_common</test_depend>

Add in CMakeLists.txt:

find_package(ament_lint_auto REQUIRED)
ament_lint_auto_find_test_dependencies()

Gives you:

    clang-tidy, cppcheck on C++

    flake8, pylint on Python

    Copyright & license headers

Integration order in CI

colcon build --symlink-install
colcon test --event-handlers console_direct+
colcon test-result --all --verbose   # fails build if any test non-zero

Stage	Runtime
Pure logic & lint	< 1 min
Spin & message tests	20–40 s
Launch tests (8–10 nodes)	30 s–1 min
Optional simulation	▷ separate job; can take minutes

Keep the fast unit suite blocking on every pull request; run heavier sim tests nightly.
Key take-aways

    Unit means no DDS round-trip. Mock or spin nodes locally for sub-millisecond feedback.

    Message, parameter, and header tests are cheap but prevent bag/sim breakage.

    Launch tests give confidence the graph assembles correctly—huge payoff, small cost.

    Block merges on lint + unit; run sim/hardware loops in later pipeline stages.

Follow this pyramid and you’ll catch 90 % of defects before they ever reach the bench, let alone the field.


A 360-degree testing matrix for a robotics project

Think of testing as concentric safety nets that catch problems earlier and more cheaply the closer they are to the center. For an autonomous robot those nets span from millisecond-level unit checks all the way to multi-hour field trials.
Ring	Focus	Typical scope	Failure cost	Main tools
1 Unit	Individual functions / classes	10–100 LOC	⬤ pennies (compile-time)	GoogleTest, pytest, ament_lint
2 Node-internal	One ROS node’s callbacks & timers	100–1 k LOC	⬤ cents	rclcpp/rclpy test executors
3 Package integration	Nodes that live in one package sharing topics/params	1–5 k LOC	⬤ dollars	launch_testing, faker publishers
4 Workspace (graph) sim	Full ROS graph driving a physics sim	10–50 k LOC	⬤ tens of dollars	Gazebo/IsaacSim + ros_gz_bridge
5 Hardware-in-the-loop (HIL)	Real sensors, actuators disabled/sandboxed	Adds wiring	⬤ hundreds	Bench jig, power-limited drivers
6 Subsystem field	Robot performs a limited mission in controlled area	Entire stack	⬤ thousands	Bags, diagnostics, tracepoints
7 End-to-end acceptance	Hours-long mission, real users, safety rules	Whole product	⬤ tens of thousands	Test protocols, KPIs, incident logs

The further out you catch a defect, the more it hurts—so invest heavily in the inner rings.
What each ring tries to prove
Ring	Key questions answered	Example checks
1 Unit	Math right? edge cases? exceptions?	PID step response, reward function, CRC parser
2 Node-internal	Callback race-free? params parsed?	IMU callback handles NaNs; DRL node reloads checkpoint
3 Package	Topics match? QoS correct?	driver fake publishes → filter node outputs expected cloud
4 Graph sim	Robot succeed in virtual world? latency OK?	Nav2 completes path in Gazebo; collision avoidance stays >0.3 m
5 HIL	Timing & wiring OK?	Camera frames arrive at 30 fps; motor watchdog trips after 200 ms silence
6 Field	Mission KPIs met?	Warehouse pick & place in 15 min; zero emergency stops
7 Acceptance	Reg-safety, user UX?	IEC 61508 stop-distance; battery swap ≤ 3 min; logs secure
Automating the rings

    Continuous Integration (CI)
    Runs rings 1-3 on every push.

        Docker or GitHub Actions with colcon cache

        colcon test, launch_testing, clang-tidy, flake8

    Nightly Simulation
    Ring 4 nightly, ring 5 weekly

        Sim replay of recorded bags; performance budget checks

        HIL bench rig triggered by cron or “merge-to-main”

    Release Candidate (RC) Pipeline
    Freeze code → automatic build → hardware rollout script

        colcon bundle → .deb/OCI → OTA staging fleet

        Prometheus/Grafana dashboards watch KPIs for 24 h

    Field Trials & Canary Robots
    Subset of robots get the build first

        Autologger + remote bag upload

        Rollback switch over SSH / serial console

Testing artefacts & metrics to track
Artefact	Why it matters
Unit-coverage report	Keeps algorithmic code at ≥ 80 % lines executed
Latency histograms (ros2 trace)	Ensure worst-case cmd → actuator ≤ N ms
Message drop counters	DDS lost samples; sensor frame-drop noticed before operators do
Energy log	Detect regressions that cut battery life
MTBF trend	Mean-time-between-faults from ops logs—drives release criteria
Best-practice tips

    Fail fast: put assertion guards in nodes (rclcpp::exceptions::throw_from_rcl_error etc.) so unexpected states abort and surface in CI.

    Synthetic but realistic data: use rosbag recorded from the robot; play at 1.2× speed to stress timing.

    Test doubles: every driver offers *_fake node compiled only in test builds.

    Deterministic seeds: ML inference, DRL exploration, random planners—all seeded so CI is repeatable.

    Safety shadowing: in HIL, actuators get a shadow topic (/cmd_vel_safe) muxed through a watchdog—software faults never energize hardware unchecked.

    Metrics gates: CI fails if sim run exceeds CPU or memory budgets by ≥ 20 %.

    Human-readable criteria: define “done” in plaintext (battery lasts 3 h indoor loop, no obstacle contacts) so testers and engineers share expectations.

One-glance timeline for a new feature

    Dev laptop: add unit tests → green

    Push: CI passes (rings 1-3)

    Nightly: sim scenario passes (ring 4)

    Friday bench: HIL passes (ring 5)

    Monday field: sandbox arena demo passes (ring 6)

    Release: fleet canary passes in production (ring 7) —> rollout

That discipline lets you add complex DRL logic, perception upgrades, or power-train tweaks without ever “bricking” a physical robot—or your team’s weekend.
